{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport gzip\nimport pandas as pd\nX=list()\nfor dirname,_,filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        path=os.path.join(dirname,filename)\n        if path.endswith('Dataset.csv'):\n            X.append(path)\n'''\nwith gzip.open(X[3],'r') as f:\n    data=f.read()\ndata=str(data)\n'''\nX=pd.read_csv(X[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nlemmatizer=WordNetLemmatizer()\ntokenizer=nltk.tokenize.TreebankWordTokenizer()\n#stemmer=PorterStemmer()\ndef convert(review):\n    tokens=tokenizer.tokenize(review)\n    #import spacy\n    #nlp=spacy.load('en',disable=['parser','ner'])\n    #XY=nlp(' '.join(tokens))\n    def get_wordnet_pos(word):\n        tag = nltk.pos_tag([word])[0][1][0].upper()\n        tag_dict = {\"J\": wordnet.ADJ,\n                    \"N\": wordnet.NOUN,\n                    \"V\": wordnet.VERB,\n                    \"R\": wordnet.ADV}\n        return tag_dict.get(tag, wordnet.NOUN)\n    Y=[lemmatizer.lemmatize(token,get_wordnet_pos(token)) for token in tokens]\n    return ' '.join(Y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"X_train=pd.DataFrame(index=range(len(X['review'])),columns=['review','sentiment'])\nfor _ in range(len(X['review'])):\n    if _%100==0:\n        print(_)\n    X_train['review'][_]=convert(X['review'][_])\n    X_train['sentiment'][_]=X['sentiment'][_]\nX_train=X_train.sample(frac=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer=TfidfVectorizer(max_df=0.7,strip_accents='ascii',ngram_range=(1,2))\nfeatures=vectorizer.fit_transform(X_train['review'])\nX_features=pd.DataFrame(features.todense(),columns=vectorizer.get_feature_names())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense,Input\nfrom keras.optimizers import Adam\ninp_size=len(X_features.columns)\nmodel=Sequential()\nmodel.add(Input(shape=(inp_size,)))\nmodel.add(Dense(1,activation='sigmoid'))\noptimizer=Adam(lr=0.001)\nmodel.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['binary_accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nx=np.array(features.todense())\ny=np.array([1 if X_train['sentiment'][_]=='positive' else 0 for _ in range(len(X['review']))])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x,y,validation_split=0.1,verbose=2,epochs=10000,shuffle=True,batch_size=256)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install google.colab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    import google.colab\n    IN_COLAB = True\nexcept:\n    IN_COLAB = False\n\nif IN_COLAB:\n    ! wget https://raw.githubusercontent.com/hse-aml/natural-language-processing/master/setup_google_colab.py -O setup_google_colab.py\n    import setup_google_colab\n    setup_google_colab.setup_week1() \n    \nimport sys\nsys.path.append(\"..\")\nfrom common.download_utils import download_week1_resources\n\ndownload_week1_resources()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom ast import literal_eval\nimport numpy as np\nimport pandas as pd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def read_data(filename):\n    X=pd.read_csv(filename,sep='\\t')\n    #X['tags']=X['tags'].apply(lambda s: '\\\"'+s+'\\\"')\n    X['tags']=X['tags'].apply(literal_eval)\n    return X\n\ntr_data=read_data('data/train.tsv')\nval_data=read_data('data/validation.tsv')\nte_data=pd.read_csv('data/test.tsv',sep='\\t')\n\nX_train,y_train=tr_data['title'].values,tr_data['tags'].values\nX_val,y_val=val_data['title'].values,val_data['tags'].values\nX_test=te_data['title'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import re\n\nREPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\n\ndef text_prepare(text):\n    text = text.lower()\n    text = REPLACE_BY_SPACE_RE.sub(' ',text)\n    text = BAD_SYMBOLS_RE.sub('',text)\n    text = ' '.join([_ if _ not in STOPWORDS else '' for _ in text.split(' ')])\n    return re.sub('\\s+',' ',text).strip()\n\ndef test_text_prepare():\n    examples = [\"SQL Server - any equivalent of Excel's CHOOSE function?\",\n                \"How to free c++ memory vector<int> * arr?\"]\n    answers = [\"sql server equivalent excels choose function\", \n               \"free c++ memory vectorint arr\"]\n    for ex, ans in zip(examples, answers):\n        if text_prepare(ex) != ans:\n            return \"Wrong answer for the case: '%s'\" % ex\n    return 'Basic tests are passed.'\nprint(test_text_prepare())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from grader import Grader\ngrader=Grader()\nprepared_questions = []\nfor line in open('data/text_prepare_tests.tsv', encoding='utf-8'):\n    line = text_prepare(line.strip())\n    prepared_questions.append(line)\ntext_prepare_results = '\\n'.join(prepared_questions)\n\ngrader.submit_tag('TextPrepare', text_prepare_results)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = [text_prepare(x) for x in X_train]\nX_val = [text_prepare(x) for x in X_val]\nX_test = [text_prepare(x) for x in X_test]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nwords=list()\ntags=list()\n[[words.append(b) for b in _.split(' ')] for _ in X_train]\n[[tags.append(t) for t in _] for _ in y_train]\nmost_common_words=Counter(words).most_common(3)\nmost_common_tags=Counter(tags).most_common(3)\ngrader.submit_tag('WordsTagsCount', '%s\\n%s' % (','.join(tag for tag, _ in most_common_tags),','.join(word for word, _ in most_common_words)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nDICT_SIZE = 5000\nWORDS_TO_INDEX=dict()\nINDEX_TO_WORDS=dict()\nW=Counter(words).most_common(DICT_SIZE)\nfor w,idx in zip(W,range(0,DICT_SIZE)):\n    WORDS_TO_INDEX[w[0]]=idx\n    INDEX_TO_WORDS[idx]=w[0]\nALL_WORDS = WORDS_TO_INDEX.keys()\n\ndef my_bag_of_words(text, words_to_index, dict_size):\n    result_vector = np.zeros(dict_size)\n    for _ in text.split(' '):\n        if _ in words_to_index.keys():\n            result_vector[words_to_index[_]]=1\n    return result_vector\n\ndef test_my_bag_of_words():\n    words_to_index = {'hi': 0, 'you': 1, 'me': 2, 'are': 3}\n    examples = ['hi how are you']\n    answers = [[1, 1, 0, 1]]\n    for ex, ans in zip(examples, answers):\n        if (my_bag_of_words(ex, words_to_index, 4) != ans).any():\n            return \"Wrong answer for the case: '%s'\" % ex\n    return 'Basic tests are passed.'\n\nprint(test_my_bag_of_words())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy import sparse as sp_sparse\nX_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\nX_val_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_val])\nX_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\nprint('X_train shape ', X_train_mybag.shape)\nprint('X_val shape ', X_val_mybag.shape)\nprint('X_test shape ', X_test_mybag.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"row = X_train_mybag[10].toarray()[0]\nnon_zero_elements_count = np.count_nonzero(row)\ngrader.submit_tag('BagOfWords', str(non_zero_elements_count))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\ndef tfidf_features(X_train, X_val, X_test):\n    tfidf_vectorizer=TfidfVectorizer(min_df=5,stop_words='english',max_df=0.9,token_pattern='(\\S+)',ngram_range=(1,3))\n    X_train=tfidf_vectorizer.fit_transform(X_train)\n    X_val=tfidf_vectorizer.transform(X_val)\n    X_test=tfidf_vectorizer.transform(X_test)\n    return X_train, X_val, X_test, tfidf_vectorizer.vocabulary_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_tfidf, X_val_tfidf, X_test_tfidf, tfidf_vocab = tfidf_features(X_train, X_val, X_test)\ntfidf_reversed_vocab = {i:word for word,i in tfidf_vocab.items()}\nlen(tfidf_vocab)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_count=Counter(tags)\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb=MultiLabelBinarizer(classes=sorted(tags_count.keys()))\ny_train=mlb.fit_transform(y_train)\ny_val=mlb.fit_transform(y_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression, RidgeClassifier, SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\ndef train_classifier(X_train,y_train):\n    #ovrc=OneVsRestClassifier(LogisticRegression(solver='lbfgs',penalty='l2',max_iter=1000,multi_class='ovr',verbose=2,n_jobs=-1),n_jobs=-1)\n    ovrc=OneVsRestClassifier(SGDClassifier(alpha=0.00001,l1_ratio=0.1,penalty='elasticnet',random_state=0),n_jobs=-1)\n    #ovrc=OneVsRestClassifier(LinearSVC(),n_jobs=-1)\n    #ovrc=OneVsRestClassifier(MultinomialNB(fit_prior=True,class_prior=None),n_jobs=-1)\n    #ovrc=OneVsRestClassifier(RidgeClassifier(max_iter=1000),n_jobs=-1)\n    ovrc=ovrc.fit(X_train,y_train)\n    return ovrc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classifier_mybag = train_classifier(X_train_mybag, y_train)\nclassifier_tfidf = train_classifier(X_train_tfidf, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_predicted_labels_mybag = classifier_mybag.predict(X_val_mybag)\n#y_val_predicted_scores_mybag = classifier_mybag.decision_function(X_val_mybag)\n\ny_val_predicted_labels_tfidf = classifier_tfidf.predict(X_val_tfidf)\n#y_val_predicted_scores_tfidf = classifier_tfidf.decision_function(X_val_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_val_pred_inversed = mlb.inverse_transform(y_val_predicted_labels_mybag)\ny_val_inversed = mlb.inverse_transform(y_val)\nfor i in range(3):\n    print('Title:\\t{}\\nTrue labels:\\t{}\\nPredicted labels:\\t{}\\n\\n'.format(\n        X_val[i],\n        ','.join(y_val_inversed[i]),\n        ','.join(y_val_pred_inversed[i])\n    ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n#from sklearn.metrics import roc_auc_score \nfrom sklearn.metrics import average_precision_score\n#from sklearn.metrics import recall_score\ndef print_evaluation_scores(y_val, predicted):\n    print('Accuracy Score: {} F1 Score: {} Precision Score: {}'.format(accuracy_score(y_val,predicted),f1_score(y_val,predicted,average='macro'),average_precision_score(y_val,predicted,average='macro')))\nprint('Bag-of-words')\nprint_evaluation_scores(y_val, y_val_predicted_labels_mybag)\nprint('Tfidf')\nprint_evaluation_scores(y_val, y_val_predicted_labels_tfidf)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_predictions=classifier_tfidf.predict(X_test_tfidf)\n'''\nwin_size=X_train_mybag.shape[1]\nnumber_of_reps=0\nfor x_t in X_test_mybag:\n    idx_array=np.nonzero(X_train_mybag==x_t)[0]\n    if idx_array.size>0:\n        for i in range(idx_array.size-win_size+1):\n            win=idx_array[i:i+win_size].copy()\n            tmp=win.copy()\n            tmp.fill(win[0])\n            if np.all(tmp==win):\n                number_of_reps+=1\n                test_predictions[win[0]]=y_train[win[0]]\nprint(number_of_reps)\n'''\n#No overlapping between Train set and Test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"'''\nimport scipy.sparse as sp\nX_train_mybag=sp.vstack((X_train_mybag,X_test_mybag),format='csr')\ny_train=np.concatenate((y_train,classifier_mybag.predict(X_test_mybag)),axis=0)\n'''\ntest_pred_inversed = mlb.inverse_transform(test_predictions)\ntest_predictions_for_submission = '\\n'.join('%i\\t%s' % (i, ','.join(row)) for i, row in enumerate(test_pred_inversed))\ngrader.submit_tag('MultilabelClassification', test_predictions_for_submission)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"STUDENT_EMAIL='mukherjeetuhin50@gmail.com'\nSTUDENT_TOKEN='oZ8QTJcofzijz7PE'\ngrader.submit(STUDENT_EMAIL,STUDENT_TOKEN)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}